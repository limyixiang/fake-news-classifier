{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd872db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "dataset = \"chengxuphd/liar2\"\n",
    "dataset = load_dataset(dataset)\n",
    "\n",
    "pretrained_model = \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    combined_input = [\n",
    "        \"Subject: \" + (subject if subject is not None else \"\") + \n",
    "        \"; Speaker: \" + (speaker if speaker is not None else \"\") + \n",
    "        \"; Speaker Description: \" + (speaker_description if speaker_description is not None else \"\") + \n",
    "        \"; State: \" + (state_info if state_info is not None else \"\") + \n",
    "        \"; Context: \" + (context if context is not None else \"\") + \n",
    "        \"; Statement: \" + (statement if statement is not None else \"\")\n",
    "        for subject, speaker, speaker_description, state_info, context, statement in zip(\n",
    "            examples[\"subject\"],\n",
    "            examples[\"speaker\"],\n",
    "            examples[\"speaker_description\"],\n",
    "            examples[\"state_info\"],\n",
    "            examples[\"context\"],\n",
    "            examples[\"statement\"]\n",
    "        )\n",
    "    ]\n",
    "    return tokenizer(combined_input, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"id\", \"subject\", \"speaker\", \"speaker_description\", \"state_info\", \"context\", \"true_counts\", \"mostly_true_counts\", \"half_true_counts\", \"mostly_false_counts\", \"false_counts\", \"pants_on_fire_counts\", \"justification\"])\n",
    "\n",
    "label_to_binary = {\n",
    "    # label_to_id = {\"REAL\": 0, \"FAKE\": 1}\n",
    "    0: True,\n",
    "    1: True,\n",
    "    2: True,\n",
    "    3: True, # Changed to FAKE\n",
    "    4: False,\n",
    "    5: False\n",
    "}\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda examples: {\"label\": [label_to_binary[int(label)] for label in examples[\"label\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Calculate normalized class weights\n",
    "count0 = sum(1 for label in train_dataset[\"label\"] if label == 0)\n",
    "count1 = sum(1 for label in train_dataset[\"label\"] if label == 1)\n",
    "total = count0 + count1\n",
    "n_classes = 2\n",
    "weight_0 = total / (n_classes * count0)\n",
    "weight_1 = total / (n_classes * count1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor([weight_0, weight_1]).to(device)\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# Define training arguments (epoch)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"recall\",  # Specify the metric to monitor\n",
    "    greater_is_better=True       # Specify if higher values of the metric are better\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model, num_labels=2, hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Swap the labels in the model configuration\n",
    "model.config.id2label = {0: \"REAL\", 1: \"FAKE\"}\n",
    "model.config.label2id = {\"REAL\": 0, \"FAKE\": 1}\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Updated id2label mapping:\", model.config.id2label)\n",
    "print(\"Updated label2id mapping:\", model.config.label2id)\n",
    "\n",
    "training_args.num_train_epochs = 5\n",
    "training_args.learning_rate = 2e-5\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "print(\"Learning rate:\", training_args.learning_rate)\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e627499",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"/kaggle/working/results/checkpoint-1725\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/results/checkpoint-1725\")\n",
    "trainer.model = model\n",
    "trainer.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct testing on the test dataset\n",
    "test_results = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predictions and metrics\n",
    "predictions = test_results.predictions.argmax(-1)  # Convert logits to class predictions\n",
    "metrics = test_results.metrics  # Contains accuracy, F1, precision, recall, etc.\n",
    "\n",
    "# Print metrics\n",
    "print(\"Test Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f47997",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models/weighted_loss/roberta\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
