{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = \"./bert/without_icl/roberta_new\"\n",
    "model_path = \"./bert/models/weighted_loss_roberta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f979455e5d074ba4a5175c8770f69ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before label preprocessing: Labels = Counter({1: 660, 3: 371, 2: 360, 4: 343, 0: 303, 5: 259})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a028c013e1459892557aac85d4cdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After label preprocessing: Labels = Counter({1: 1694, 0: 602})\n"
     ]
    }
   ],
   "source": [
    "dataset = \"chengxuphd/liar2\"\n",
    "dataset = load_dataset(dataset)\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    combined_input = [\n",
    "        \"Subject: \" + (subject if subject is not None else \"\") + \n",
    "        \"; Speaker: \" + (speaker if speaker is not None else \"\") + \n",
    "        \"; Speaker Description: \" + (speaker_description if speaker_description is not None else \"\") + \n",
    "        \"; State: \" + (state_info if state_info is not None else \"\") + \n",
    "        \"; Context: \" + (context if context is not None else \"\") + \n",
    "        \"; Statement: \" + (statement if statement is not None else \"\")\n",
    "        for subject, speaker, speaker_description, state_info, context, statement in zip(\n",
    "            examples[\"subject\"],\n",
    "            examples[\"speaker\"],\n",
    "            examples[\"speaker_description\"],\n",
    "            examples[\"state_info\"],\n",
    "            examples[\"context\"],\n",
    "            examples[\"statement\"]\n",
    "        )\n",
    "    ]\n",
    "    return tokenizer(combined_input, padding=\"max_length\", truncation=True)\n",
    "\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "label_to_binary = {\n",
    "    # True = FAKE; False = REAL\n",
    "    0: True,\n",
    "    1: True,\n",
    "    2: True,\n",
    "    3: True, # Changed to FAKE\n",
    "    4: False,\n",
    "    5: False\n",
    "}\n",
    "\n",
    "original_label_counts = Counter(test_dataset[\"label\"].tolist())\n",
    "print(\"Before label preprocessing: Labels =\", original_label_counts)\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda examples: {\"label\": [label_to_binary[int(label)] for label in examples[\"label\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "binary_label_counts = Counter(test_dataset[\"label\"].tolist())\n",
    "print(\"After label preprocessing: Labels =\", binary_label_counts)\n",
    "\n",
    "# assert (\n",
    "#     original_label_counts[0] + original_label_counts[1] + original_label_counts[2]\n",
    "#     == binary_label_counts[True]\n",
    "# ), \"Sum of original labels 0, 1, 2 does not match new label 0 (Fake).\"\n",
    "\n",
    "# assert (\n",
    "#     original_label_counts[3] + original_label_counts[4] + original_label_counts[5]\n",
    "#     == binary_label_counts[False]\n",
    "# ), \"Sum of original labels 3, 4, 5 does not match new label 1 (Real).\"\n",
    "\n",
    "# print(\"Assertions passed: Label mapping is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "test_loss: 0.4677\n",
      "test_model_preparation_time: 0.0020\n",
      "test_accuracy: 0.7404\n",
      "test_f1: 0.8218\n",
      "test_precision: 0.8327\n",
      "test_recall: 0.8111\n",
      "test_runtime: 33.7047\n",
      "test_samples_per_second: 68.1210\n",
      "test_steps_per_second: 8.5150\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "test_results = trainer.predict(test_dataset)\n",
    "\n",
    "predictions = test_results.predictions.argmax(-1)  # Convert logits to class predictions\n",
    "metrics = test_results.metrics  # Contains accuracy, F1, precision, recall, etc.\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling\n",
    "\n",
    "| Metric                        | Regular | Weighted_loss   |\n",
    "|-------------------------------|---------|-----------------|\n",
    "| Test Loss                     | 0.6739  | 0.4677          |\n",
    "| Test Model Preparation Time   | 0.0020  | 0.0020          |\n",
    "| Test Accuracy                 | 0.6995  | 0.7404          |\n",
    "| Test F1 Score                 | 0.7702  | 0.8218          |\n",
    "| Test Precision                | 0.8838  | 0.8327          |\n",
    "| Test Recall                   | 0.6824  | 0.8111          |\n",
    "| Test Runtime (seconds)        | 37.4584 | 33.7047         |\n",
    "| Test Samples/Second           | 61.2950 | 68.1210         |\n",
    "| Test Steps/Second             | 7.6620  | 8.5150          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AFTER LABEL SWAP\n",
    "\n",
    "##### WITHOUT IN-CONTEXT LEARNING JUSTIFICATION\n",
    "\n",
    "| Metric                                | roberta   | bert_fake_news    | augmented_normal  | normal_augmented  |\n",
    "|---------------------------------------|-----------|-------------------|-------------------|-------------------|\n",
    "| Test Loss                             | 0.5207    | 0.5508            | 0.5444            | 0.5291            |\n",
    "| Test Model Preparation Time (seconds) | 0.0020    | 0.0020            | 0.0020            | 0.0010            |\n",
    "| Test Accuracy                         | 0.7295    | 0.7317            | 0.7326            | 0.7348            |\n",
    "| Test F1 Score                         | 0.7663    | 0.7548            | 0.7607            | 0.7505            |\n",
    "| Test Precision                        | 0.7631    | 0.7973            | 0.7852            | 0.8193            |\n",
    "| Test Recall                           | 0.7695    | 0.7166            | 0.7377            | 0.6924            |\n",
    "| Test Runtime (seconds)                | 38.4320   | 36.0601           | 34.2745           | 33.9310           |\n",
    "| Test Samples/Second                   | 59.7420   | 63.6720           | 66.9880           | 67.6670           |\n",
    "| Test Steps/Second                     | 7.4680    | 7.9590            | 8.3740            | 8.4580            |\n",
    "\n",
    "Since roberta provided a better recall score, we shall compare data augmentation on the roberta model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEFORE LABEL SWAP\n",
    "\n",
    "| Metric                                | roberta   | augmented_normal  | normal_augmented  | justification | bert_fake_news    |\n",
    "|---------------------------------------|-----------|-------------------|-------------------|---------------|-------------------|\n",
    "| Test Loss                             | 0.5339    | 0.6042            | 0.5393            | 0.7983        | 0.5354            |\n",
    "| Test Model Preparation Time (seconds) | 0.0010    | 0.0020            | 0.0020            | 0.0010        | 0.0020            |\n",
    "| Test Accuracy                         | 0.7395    | 0.7352            | 0.7260            | 0.6760        | 0.7317            |\n",
    "| Test F1 Score                         | 0.7051    | 0.7214            | 0.7137            | 0.6958        | 0.7100            |\n",
    "| Test Precision                        | 0.6777    | 0.6510            | 0.6405            | 0.5777        | 0.6551            |\n",
    "| Test Recall                           | 0.7348    | 0.8088            | 0.8058            | 0.8746        | 0.7749            |\n",
    "| Test Runtime (seconds)                | 33.5466   | 33.379            | 33.718            | 33.415        | 33.5630           |\n",
    "| Test Samples/Second                   | 68.4420   | 68.785            | 68.094            | 68.710        | 68.4090           |\n",
    "| Test Steps/Second                     | 8.5550    | 8.5980            | 8.5120            | 8.5890        | 8.5510            |\n",
    "\n",
    "Roberta seems to be the best model out of all 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
