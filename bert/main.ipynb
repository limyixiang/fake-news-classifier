{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall transformers torch scikit-learn gensim datasets -q\n",
    "# import transformers, torch, sklearn, gensim, datasets\n",
    "# print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 30 20:22:03 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.83                 Driver Version: 572.83         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   47C    P8            N/A  /  115W |    7501MiB /   8188MiB |      8%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            4460    C+G   ....270.0.12\\OverwolfBrowser.exe      N/A      |\n",
      "|    0   N/A  N/A            5536    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A            5592    C+G   ...em_tray\\lghub_system_tray.exe      N/A      |\n",
      "|    0   N/A  N/A            6600    C+G   ...56_x64__ttt1ap7aakyb4\\Arc.exe      N/A      |\n",
      "|    0   N/A  N/A            7640    C+G   ...al\\Programs\\Notion\\Notion.exe      N/A      |\n",
      "|    0   N/A  N/A            8552    C+G   ...cord\\app-1.0.9187\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A            9236    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A            9300    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A            9652    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "|    0   N/A  N/A           11176    C+G   ...Telegram Desktop\\Telegram.exe      N/A      |\n",
      "|    0   N/A  N/A           11432    C+G   ...s\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A           12180    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           12204    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           13248    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           13720    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           14348    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           15160    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           16552    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           16612    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           17532    C+G   ...56_x64__ttt1ap7aakyb4\\Arc.exe      N/A      |\n",
      "|    0   N/A  N/A           18484    C+G   ...s (x86)\\Overwolf\\Overwolf.exe      N/A      |\n",
      "|    0   N/A  N/A           19376    C+G   ...aries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A           19536    C+G   ...rvices\\BlueStacksServices.exe      N/A      |\n",
      "|    0   N/A  N/A           19956    C+G   ...AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A           23548    C+G   ...64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A           23968    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A           24028    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           24148    C+G   ...4__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A           26704    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           29428      C   ...s\\Python\\Python311\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\limyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\limyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\limyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6727b5c4c87c4772a0726ca8b140fd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6560168c04646e0950fc054ba04be4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the LIAR dataset\n",
    "dataset = load_dataset(\"liar\", trust_remote_code=True)\n",
    "\n",
    "# Define the saved model directory\n",
    "saved_model_dir = \"./saved_model_2\"\n",
    "\n",
    "# Check if the saved tokenizer exists\n",
    "if os.path.exists(saved_model_dir):\n",
    "    print(\"Loading saved tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(saved_model_dir)\n",
    "else:\n",
    "    print(\"Saved tokenizer not found. Loading default BERT tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Tokenization function\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"statement\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Preprocessing function for text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove numbers and punctuation\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Stopword removal\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    \n",
    "    return \" \".join(tokens) if tokens else \"\"  # Return the processed text as a string\n",
    "\n",
    "# Combine metadata with the preprocessed statement\n",
    "def preprocess_function(examples):\n",
    "    combined_input = [\n",
    "        \"Subject: \" + subject + \n",
    "        \"; Speaker: \" + speaker + \n",
    "        \"; Job Title: \" + job_title + \n",
    "        \"; State: \" + state_info + \n",
    "        \"; Party: \" + party_affiliation + \n",
    "        \" Statement: \" + preprocess_text(statement)  # Apply preprocess_text here\n",
    "        for subject, speaker, job_title, state_info, party_affiliation, statement in zip(\n",
    "            examples[\"subject\"],\n",
    "            examples[\"speaker\"],\n",
    "            examples[\"job_title\"],\n",
    "            examples[\"state_info\"],\n",
    "            examples[\"party_affiliation\"],\n",
    "            examples[\"statement\"]\n",
    "        )\n",
    "    ]\n",
    "    return tokenizer(combined_input, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns and set format for PyTorch\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"id\", \"subject\", \"speaker\", \"job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Preprocess labels to binary True / False\n",
    "label_to_binary = {\n",
    "    'false': False,\n",
    "    'half-true': True,\n",
    "    'mostly-true': True,\n",
    "    'true': True,\n",
    "    'barely-true': False,\n",
    "    'pants-fire': False\n",
    "}\n",
    "\n",
    "# Access label names\n",
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "# Apply binary label preprocessing\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda examples: {\"labels\": [label_to_binary[label_names[label]] for label in examples[\"labels\"]]},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model (2 classes for binary classification)\n",
    "# Check if the saved model exists\n",
    "if os.path.exists(saved_model_dir):\n",
    "    print(\"Loading saved model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(saved_model_dir)\n",
    "else:\n",
    "    print(\"Saved model not found. Loading default BERT model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# Check which device your model is on\n",
    "print(\"Model is on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\limyi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# Define training arguments (epoch)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Specify the metric to monitor\n",
    "    greater_is_better=False       # Specify if higher values of the metric are better\n",
    ")\n",
    "\n",
    "# # Define training arguments (steps for smaller batch logging)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"steps\",  # Evaluate during training\n",
    "#     eval_steps=100,               # Evaluate every 100 steps\n",
    "#     save_strategy=\"steps\",        # Save checkpoints every 100 steps\n",
    "#     save_steps=100,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     learning_rate=2e-5,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,             # Log every 10 steps\n",
    "#     report_to=\"none\",\n",
    "#     load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "#     metric_for_best_model=\"f1\",  # Specify the metric to monitor\n",
    "#     greater_is_better=True       # Specify if higher values of the metric are better\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Train on small subset of data\n",
    "# small_train_dataset = train_dataset.select(range(500))  # Use only 100 samples\n",
    "# small_val_dataset = val_dataset.select(range(250))      # Use only 50 samples\n",
    "\n",
    "# train_labels = [label.item() for label in small_train_dataset[\"labels\"]]\n",
    "# val_labels = [label.item() for label in small_val_dataset[\"labels\"]]\n",
    "# print(\"Train label distribution:\", Counter(train_labels))\n",
    "# print(\"Validation label distribution:\", Counter(val_labels))\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=small_val_dataset,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_25648\\497997799.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/642 03:22 < 07:32, 0.98 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.669745</td>\n",
       "      <td>0.594237</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.712575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.663912</td>\n",
       "      <td>0.589564</td>\n",
       "      <td>0.608178</td>\n",
       "      <td>0.604136</td>\n",
       "      <td>0.612275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_25648\\497997799.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/642 06:43 < 04:05, 0.99 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.682600</td>\n",
       "      <td>0.665061</td>\n",
       "      <td>0.595016</td>\n",
       "      <td>0.593114</td>\n",
       "      <td>0.621311</td>\n",
       "      <td>0.567365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.663452</td>\n",
       "      <td>0.604361</td>\n",
       "      <td>0.643258</td>\n",
       "      <td>0.605820</td>\n",
       "      <td>0.685629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.663080</td>\n",
       "      <td>0.617601</td>\n",
       "      <td>0.670248</td>\n",
       "      <td>0.607795</td>\n",
       "      <td>0.747006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.638300</td>\n",
       "      <td>0.654776</td>\n",
       "      <td>0.615265</td>\n",
       "      <td>0.652113</td>\n",
       "      <td>0.615691</td>\n",
       "      <td>0.693114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_25648\\497997799.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/642 05:01 < 05:46, 0.99 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.655100</td>\n",
       "      <td>0.658413</td>\n",
       "      <td>0.605140</td>\n",
       "      <td>0.570703</td>\n",
       "      <td>0.656920</td>\n",
       "      <td>0.504491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.666059</td>\n",
       "      <td>0.602804</td>\n",
       "      <td>0.690158</td>\n",
       "      <td>0.580777</td>\n",
       "      <td>0.850299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.652687</td>\n",
       "      <td>0.621495</td>\n",
       "      <td>0.663435</td>\n",
       "      <td>0.617268</td>\n",
       "      <td>0.717066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_25648\\497997799.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/642 06:42 < 04:04, 0.99 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.662033</td>\n",
       "      <td>0.605140</td>\n",
       "      <td>0.625830</td>\n",
       "      <td>0.617176</td>\n",
       "      <td>0.634731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.669500</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.591928</td>\n",
       "      <td>0.790419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.641900</td>\n",
       "      <td>0.670926</td>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.695441</td>\n",
       "      <td>0.585466</td>\n",
       "      <td>0.856287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.648087</td>\n",
       "      <td>0.640966</td>\n",
       "      <td>0.680969</td>\n",
       "      <td>0.633205</td>\n",
       "      <td>0.736527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 5e-05, Best F1: 0.6954407294832827\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-5, 2e-5, 3e-5, 5e-5]\n",
    "best_lr = None\n",
    "best_f1 = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Reinitialize model for each lr\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    print(\"Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "    training_args.learning_rate = lr\n",
    "    training_args.num_train_epochs = 1\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    if metrics[\"eval_f1\"] > best_f1:\n",
    "        best_f1 = metrics[\"eval_f1\"]\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"Best learning rate: {best_lr}, Best F1: {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some key observations:\n",
    "\n",
    "#### Lower Learning Rate (1e-5):\n",
    "\n",
    "| Step | Training Loss | Validation Loss | Accuracy | F1      | Precision | Recall  |\n",
    "|------|---------------|-----------------|----------|---------|-----------|---------|\n",
    "| 100  | 0.687000      | 0.669745        | 0.594237 | 0.646300| 0.591304  | 0.712575|\n",
    "| 200  | 0.672300      | 0.663912        | 0.589564 | 0.608178| 0.604136  | 0.612275|\n",
    "\n",
    "The improvements in training and validation loss are slow, and the F1 score actually drops slightly from step 100 to 200.\n",
    "\n",
    "This suggests that the learning rate might be too low, so the model is learning slowly and may not be making significant progress.\n",
    "\n",
    "#### Moderate Learning Rate (2e-5):\n",
    "\n",
    "| Step | Training Loss | Validation Loss | Accuracy | F1      | Precision | Recall  |\n",
    "|------|---------------|-----------------|----------|---------|-----------|---------|\n",
    "| 100  | 0.682600      | 0.665061        | 0.595016 | 0.593114| 0.621311  | 0.567365|\n",
    "| 200  | 0.666800      | 0.663452        | 0.604361 | 0.643258| 0.605820  | 0.685629|\n",
    "| 300  | 0.653800      | 0.663080        | 0.617601 | 0.670248| 0.607795  | 0.747006|\n",
    "| 400  | 0.638300      | 0.654776        | 0.615265 | 0.652113| 0.615691  | 0.693114|\n",
    "\n",
    "You see a steady decrease in training loss and a relatively stable validation loss over time.\n",
    "\n",
    "The accuracy and F1 scores gradually improve (reaching around 0.67 F1 at step 300) with more training steps.\n",
    "\n",
    "This indicates that 2e-5 may be a sweet spot, allowing the model to learn effectively without too much instability.\n",
    "\n",
    "#### Slightly Higher Learning Rate (3e-5):\n",
    "\n",
    "| Step | Training Loss | Validation Loss | Accuracy | F1      | Precision | Recall  |\n",
    "|------|---------------|-----------------|----------|---------|-----------|---------|\n",
    "| 100  | 0.655100      | 0.658413        | 0.605140 | 0.570703| 0.656920  | 0.504491|\n",
    "| 200  | 0.678600      | 0.666059        | 0.602804 | 0.690158| 0.580777  | 0.850299|\n",
    "| 300  | 0.654400      | 0.652687        | 0.621495 | 0.663435| 0.617268  | 0.717066|\n",
    "\n",
    "The results are a bit more inconsistent. For example, at step 200, the recall jumps to 0.85 while precision drops, which causes a spike in the F1 scoreâ€”but this might be an unstable behavior.\n",
    "\n",
    "Overall, the performance is mixed, suggesting that 3e-5 might be on the higher end of what the model can handle reliably.\n",
    "\n",
    "#### Even Higher Learning Rate (5e-5):\n",
    "\n",
    "| Step | Training Loss | Validation Loss | Accuracy | F1      | Precision | Recall  |\n",
    "|------|---------------|-----------------|----------|---------|-----------|---------|\n",
    "| 100  | 0.651600      | 0.662033        | 0.605140 | 0.625830| 0.617176  | 0.634731|\n",
    "| 200  | 0.669500      | 0.658065        | 0.607477 | 0.676923| 0.591928  | 0.790419|\n",
    "| 300  | 0.641900      | 0.670926        | 0.609813 | 0.695441| 0.585466  | 0.856287|\n",
    "| 400  | 0.658000      | 0.648087        | 0.640966 | 0.680969| 0.633205  | 0.736527|\n",
    "\n",
    "At step 300, the F1 score reaches a peak (~0.70), and accuracy also improves slightly by step 400.\n",
    "\n",
    "However, the fluctuations in training and validation loss suggest that while the model is learning faster, it might also be more volatile or overfit in parts (indicated by a high recall sometimes paired with lower precision).\n",
    "\n",
    "#### Conclusion\n",
    "The moderate learning rate (2e-5) appears to offer a good balance by steadily reducing losses and improving accuracy and F1 without dramatic swings in precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n",
      "Learning rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_16624\\2080604947.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2568' max='3210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2568/3210 35:36 < 08:54, 1.20 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.709921</td>\n",
       "      <td>0.577103</td>\n",
       "      <td>0.700496</td>\n",
       "      <td>0.554585</td>\n",
       "      <td>0.950599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.634007</td>\n",
       "      <td>0.637072</td>\n",
       "      <td>0.631329</td>\n",
       "      <td>0.669463</td>\n",
       "      <td>0.597305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.661227</td>\n",
       "      <td>0.639408</td>\n",
       "      <td>0.681349</td>\n",
       "      <td>0.630573</td>\n",
       "      <td>0.741018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.833092</td>\n",
       "      <td>0.637072</td>\n",
       "      <td>0.682993</td>\n",
       "      <td>0.625935</td>\n",
       "      <td>0.751497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2568, training_loss=0.558171216851083, metrics={'train_runtime': 2136.7987, 'train_samples_per_second': 24.029, 'train_steps_per_second': 1.502, 'total_flos': 1.080754970996736e+16, 'train_loss': 0.558171216851083, 'epoch': 4.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert-base-uncased\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "print(\"Learning rate:\", training_args.learning_rate)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert-base-uncased (4 epoch, lr: 2e-5, before preprocessing to remove casings)\n",
    "\n",
    "| Epoch | Training Loss | Validation Loss | Accuracy | F1      | Precision | Recall   |\n",
    "|-------|---------------|-----------------|----------|---------|-----------|----------|\n",
    "| 1     | 0.639900      | 0.691008        | 0.585670 | 0.701124 | 0.561151  | 0.934132 |\n",
    "| 2     | 0.670800      | 0.622549        | 0.645639 | 0.660194 | 0.658718  | 0.661677 |\n",
    "| 3     | 0.514500      | 0.684059        | 0.651869 | 0.693205 | 0.640051  | 0.755988 |\n",
    "| 4     | 0.338500      | 0.852544        | 0.637850 | 0.686023 | 0.624846  | 0.760479 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6340070366859436,\n",
       " 'eval_model_preparation_time': 0.002,\n",
       " 'eval_accuracy': 0.6370716510903427,\n",
       " 'eval_f1': 0.6313291139240507,\n",
       " 'eval_precision': 0.6694630872483222,\n",
       " 'eval_recall': 0.5973053892215568,\n",
       " 'eval_runtime': 19.6527,\n",
       " 'eval_samples_per_second': 65.335,\n",
       " 'eval_steps_per_second': 4.122}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_848\\2791209380.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/642 03:23 < 02:03, 1.95 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686900</td>\n",
       "      <td>0.678743</td>\n",
       "      <td>0.535826</td>\n",
       "      <td>0.688610</td>\n",
       "      <td>0.528892</td>\n",
       "      <td>0.986527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.665692</td>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.646564</td>\n",
       "      <td>0.608179</td>\n",
       "      <td>0.690120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>0.664244</td>\n",
       "      <td>0.609034</td>\n",
       "      <td>0.661725</td>\n",
       "      <td>0.601716</td>\n",
       "      <td>0.735030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.657628</td>\n",
       "      <td>0.620717</td>\n",
       "      <td>0.658246</td>\n",
       "      <td>0.619551</td>\n",
       "      <td>0.702096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_848\\2791209380.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/642 05:09 < 00:21, 1.93 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686300</td>\n",
       "      <td>0.668643</td>\n",
       "      <td>0.592679</td>\n",
       "      <td>0.663666</td>\n",
       "      <td>0.581736</td>\n",
       "      <td>0.772455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.665708</td>\n",
       "      <td>0.615265</td>\n",
       "      <td>0.663488</td>\n",
       "      <td>0.608750</td>\n",
       "      <td>0.729042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.661600</td>\n",
       "      <td>0.665119</td>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.667110</td>\n",
       "      <td>0.599761</td>\n",
       "      <td>0.751497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>0.655833</td>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.632428</td>\n",
       "      <td>0.620144</td>\n",
       "      <td>0.645210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.659760</td>\n",
       "      <td>0.615265</td>\n",
       "      <td>0.663029</td>\n",
       "      <td>0.609023</td>\n",
       "      <td>0.727545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.647700</td>\n",
       "      <td>0.650710</td>\n",
       "      <td>0.616044</td>\n",
       "      <td>0.632364</td>\n",
       "      <td>0.630015</td>\n",
       "      <td>0.634731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_848\\2791209380.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/642 04:15 < 01:12, 1.95 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.663773</td>\n",
       "      <td>0.599688</td>\n",
       "      <td>0.568792</td>\n",
       "      <td>0.646947</td>\n",
       "      <td>0.507485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.675500</td>\n",
       "      <td>0.663832</td>\n",
       "      <td>0.602025</td>\n",
       "      <td>0.680824</td>\n",
       "      <td>0.584137</td>\n",
       "      <td>0.815868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.648500</td>\n",
       "      <td>0.660740</td>\n",
       "      <td>0.612928</td>\n",
       "      <td>0.677482</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>0.781437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.649400</td>\n",
       "      <td>0.653745</td>\n",
       "      <td>0.617601</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>0.621733</td>\n",
       "      <td>0.676647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.662800</td>\n",
       "      <td>0.656104</td>\n",
       "      <td>0.617601</td>\n",
       "      <td>0.670248</td>\n",
       "      <td>0.607795</td>\n",
       "      <td>0.747006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_848\\2791209380.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/642 05:04 < 00:21, 1.97 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.668259</td>\n",
       "      <td>0.600467</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.669584</td>\n",
       "      <td>0.458084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.674500</td>\n",
       "      <td>0.660157</td>\n",
       "      <td>0.623053</td>\n",
       "      <td>0.677333</td>\n",
       "      <td>0.610577</td>\n",
       "      <td>0.760479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.671197</td>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.690932</td>\n",
       "      <td>0.587618</td>\n",
       "      <td>0.838323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.655318</td>\n",
       "      <td>0.619159</td>\n",
       "      <td>0.610359</td>\n",
       "      <td>0.652470</td>\n",
       "      <td>0.573353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>0.655298</td>\n",
       "      <td>0.627726</td>\n",
       "      <td>0.679625</td>\n",
       "      <td>0.615291</td>\n",
       "      <td>0.758982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.631600</td>\n",
       "      <td>0.643826</td>\n",
       "      <td>0.630062</td>\n",
       "      <td>0.647365</td>\n",
       "      <td>0.642121</td>\n",
       "      <td>0.652695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 5e-05, Best F1: 0.6909315237507712\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-5, 2e-5, 3e-5, 5e-5]\n",
    "best_lr = None\n",
    "best_f1 = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Reinitialize model for each lr\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    print(\"Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "    training_args.learning_rate = lr\n",
    "    training_args.num_train_epochs = 1\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    if metrics[\"eval_f1\"] > best_f1:\n",
    "        best_f1 = metrics[\"eval_f1\"]\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"Best learning rate: {best_lr}, Best F1: {best_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n",
      "Learning rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_16444\\1378444068.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 09:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>0.699789</td>\n",
       "      <td>0.585670</td>\n",
       "      <td>0.700787</td>\n",
       "      <td>0.561261</td>\n",
       "      <td>0.932635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.637094</td>\n",
       "      <td>0.644081</td>\n",
       "      <td>0.683299</td>\n",
       "      <td>0.636129</td>\n",
       "      <td>0.738024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1284, training_loss=0.6465281116256832, metrics={'train_runtime': 553.1649, 'train_samples_per_second': 37.128, 'train_steps_per_second': 2.321, 'total_flos': 2720615433596928.0, 'train_loss': 0.6465281116256832, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distilbert-base-uncased\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=2, hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "training_args.num_train_epochs = 2\n",
    "training_args.learning_rate = 2e-5\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "print(\"Learning rate:\", training_args.learning_rate)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n",
      "Learning rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_24068\\3382814383.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='642' max='642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [642/642 08:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.666093</td>\n",
       "      <td>0.598910</td>\n",
       "      <td>0.636042</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.673653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=642, training_loss=0.6825153445157678, metrics={'train_runtime': 536.4309, 'train_samples_per_second': 19.143, 'train_steps_per_second': 1.197, 'total_flos': 2701887427491840.0, 'train_loss': 0.6825153445157678, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", num_labels=2, ignore_mismatched_sizes=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "training_args.num_train_epochs = 3\n",
    "training_args.learning_rate = 2e-5\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "print(\"Learning rate:\", training_args.learning_rate)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n",
      "Learning rate: 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_16444\\2134050097.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1926' max='1926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1926/1926 1:07:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.694289</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>0.684426</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>0.684426</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.659200</td>\n",
       "      <td>0.682945</td>\n",
       "      <td>0.554517</td>\n",
       "      <td>0.670127</td>\n",
       "      <td>0.545028</td>\n",
       "      <td>0.869760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1926, training_loss=0.6847655094796375, metrics={'train_runtime': 4042.7724, 'train_samples_per_second': 7.62, 'train_steps_per_second': 0.476, 'total_flos': 8105662282475520.0, 'train_loss': 0.6847655094796375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xlm-roberta-base (wasted capacity since xlm-roberta is a multilingual model trained on 100 languages)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "training_args.num_train_epochs = 3\n",
    "training_args.learning_rate = 2e-5\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "print(\"Learning rate:\", training_args.learning_rate)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_16444\\563495023.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "test_loss: 0.6432\n",
      "test_model_preparation_time: 0.0010\n",
      "test_accuracy: 0.6461\n",
      "test_f1: 0.7090\n",
      "test_precision: 0.6639\n",
      "test_recall: 0.7607\n",
      "test_runtime: 9.6851\n",
      "test_samples_per_second: 132.4720\n",
      "test_steps_per_second: 8.3630\n",
      "Predictions: [1 1 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# trainer.evaluate()\n",
    "\n",
    "# Conduct testing on the test dataset\n",
    "test_results = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predictions and metrics\n",
    "predictions = test_results.predictions.argmax(-1)  # Convert logits to class predictions\n",
    "metrics = test_results.metrics  # Contains accuracy, F1, precision, recall, etc.\n",
    "\n",
    "# Print metrics\n",
    "print(\"Test Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Optionally, inspect the predictions\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result so far:\n",
    "\n",
    "Model: distilbert-base-uncased\n",
    "\n",
    "Dropout: 0.3\n",
    "\n",
    "Learning rate: 2e-5\n",
    "\n",
    "Test Metrics:\n",
    "\n",
    "| Metric                     | Value     |\n",
    "|----------------------------|-----------|\n",
    "| Test Loss                 | 0.6432    |\n",
    "| Model Preparation Time    | 0.0010    |\n",
    "| Test Accuracy             | 0.6461    |\n",
    "| Test F1                   | 0.7090    |\n",
    "| Test Precision            | 0.6639    |\n",
    "| Test Recall               | 0.7607    |\n",
    "| Test Runtime              | 9.6851    |\n",
    "| Test Samples per Second   | 132.4720  |\n",
    "| Test Steps per Second     | 8.3630    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# train_labels = [label.item() for label in train_dataset[\"labels\"]]\n",
    "# val_labels = [label.item() for label in val_dataset[\"labels\"]]\n",
    "# print(\"Train label distribution:\", Counter(train_labels))\n",
    "# print(\"Validation label distribution:\", Counter(val_labels))\n",
    "\n",
    "# predictions = trainer.predict(val_dataset)\n",
    "# preds = predictions.predictions.argmax(-1)\n",
    "# print(Counter(preds))  # Check the distribution of predicted labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
